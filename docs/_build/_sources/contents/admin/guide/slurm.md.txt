# Slurm

Sources: 
https://slurm.schedmd.com/quickstart_admin.html \
https://www.slothparadise.com/how-to-install-slurm-on-centos-7-cluster/ \
https://bitsanddragons.wordpress.com/2016/08/22/slurm-on-centos-7/ \
http://sysadm.mielnet.pl/building-and-installing-rpm-slurm-on-centos-7/

- Clocks synchronized across the cluster (Not well solved. Ask Damián)

cluster_exec 'timedatectl'
cluster_exec 'ntpstat' # to detect nodes with ntpd not running
In case nptd is not running:
systemctl enable ntpd

After waiting some time everything looks syncronized with ntp server.
cluster_exec 'clock'; clock
cluster_exec 'date'; date

In some web pages syncronizing the nodes with the master is suggested. I dont need it but this should be asked to Damián.

## Installation

Only in the server
```
yum install mariadb-server mariadb-devel -y       #-IT WAS DONE-
``` 

```bash
export MUNGEUSER=988             #-IT WAS DONE (why this number?)-
export MUNGEGROUP=983            #-IT WAS DONE (why this number?)-
export CHRONYUSER=991            #-IT WAS DONE (why this number?)-
export CHRONYGROUP=988           #-IT WAS DONE (why this number?)-
export SLURMUSER=1005            #-IT WAS NOT DONE-
export SLURMGROUP=1005           #-IT WAS NOT DONE-

groupadd -g $MUNGEGROUP munge    #-IT WAS DONE-
useradd  -m -c "MUNGE Uid 'N' Gid Emporium" -d /var/lib/munge -u $MUNGEUSER -g munge  -s /sbin/nologin munge #-It WAS DONE (but it was -d /var/run/munge. I did changed to /var/lib/munge)
#chrony was already ok
chrony:x:991:988::/var/lib/chrony:/sbin/nologin
groupadd -g $SLURMGROUP slurm
useradd -m -c "SLURM workload manager"  -d /var/lib/slurm -u $SLURMUSER -g slurm -s /bin/bash slurm

cluster_update
cluster_exec 'mkdir /var/lib/munge; chown -R munge:munge /var/lib/munge'
cluster_exec 'mkdir /var/lib/slurm; chown -R slurm:slurm /var/lib/slurm'
```
``` bash
yum install munge munge-libs munge-devel -y
cluster_exec 'yum install munge munge-libs munge-devel -y'
yum install rng-tools -y
cluster_exec 'yum install rng-tools -y'

rngd -r /dev/urandom
/usr/sbin/create-munge-key -r
dd if=/dev/urandom bs=1 count=1024 > /etc/munge/munge.key
chown munge: /etc/munge/munge.key
chmod 400 /etc/munge/munge.key

cluster_exec 'scp ixtlilton:/etc/munge/munge.key /etc/munge/munge.key'
cluster_exec 'chown munge: /etc/munge/munge.key; chmod 400 /etc/munge/munge.key'
cluster_exec 'chown -R munge: /etc/munge/ /var/log/munge/ ; chmod 0700 /etc/munge/ /var/log/munge/'

systemctl enable munge
systemctl start munge

cluster_exec 'systemctl enable munge'
cluster_exec 'systemctl start munge'

```

```
yum install openssl openssl-devel pam-devel numactl numactl-devel hwloc hwloc-devel lua lua-devel readline-devel rrdtool-devel ncurses-devel man2html libibmad libibumad -y

cluster_exec 'yum install openssl openssl-devel pam-devel numactl numactl-devel hwloc hwloc-devel lua lua-devel readline-devel rrdtool-devel ncurses-devel man2html libibmad libibumad -y'

mkdir -p /opt/src/slurm/pkg/17.11.2
cd /opt/src/slurm/pkg/17.11.2
wget https://download.schedmd.com/slurm/slurm-17.11.2.tar.bz2
rpmbuild -ta slurm-16.05.4.tar.bz2
mkdir /opt/src/slurm/pkg/17.11.2/rpms
cp /root/rpmbuild/RPMS/x86_64/slurm*17.11.2-1.el7.centos.x86_64.rpm /opt/src/slurm/pkg/17.11.2/rpms/.
cd /opt/src/slurm/pkg/17.11.2/rpms/
yum --nogpgcheck localinstall \
slurm-17.11.2-1.el7.centos.x86_64.rpm \
slurm-contribs-17.11.2-1.el7.centos.x86_64.rpm \
slurm-devel-17.11.2-1.el7.centos.x86_64.rpm \
slurm-example-configs-17.11.2-1.el7.centos.x86_64.rpm \
slurm-libpmi-17.11.2-1.el7.centos.x86_64.rpm \
slurm-openlava-17.11.2-1.el7.centos.x86_64.rpm \
slurm-pam_slurm-17.11.2-1.el7.centos.x86_64.rpm \
slurm-perlapi-17.11.2-1.el7.centos.x86_64.rpm \
slurm-slurmctld-17.11.2-1.el7.centos.x86_64.rpm \
slurm-slurmd-17.11.2-1.el7.centos.x86_64.rpm \
slurm-slurmdbd-17.11.2-1.el7.centos.x86_64.rpm \
slurm-torque-17.11.2-1.el7.centos.x86_64.rpm

cluster_exec 'cd /opt/src/slurm/pkg/17.11.2/rpms; yum --nogpgcheck localinstall slurm-17.11.2-1.el7.centos.x86_64.rpm slurm-contribs-17.11.2-1.el7.centos.x86_64.rpm slurm-devel-17.11.2-1.el7.centos.x86_64.rpm slurm-example-configs-17.11.2-1.el7.centos.x86_64.rpm slurm-libpmi-17.11.2-1.el7.centos.x86_64.rpm slurm-openlava-17.11.2-1.el7.centos.x86_64.rpm slurm-pam_slurm-17.11.2-1.el7.centos.x86_64.rpm slurm-perlapi-17.11.2-1.el7.centos.x86_64.rpm slurm-slurmctld-17.11.2-1.el7.centos.x86_64.rpm slurm-slurmd-17.11.2-1.el7.centos.x86_64.rpm slurm-slurmdbd-17.11.2-1.el7.centos.x86_64.rpm slurm-torque-17.11.2-1.el7.centos.x86_64.rpm -y'
```

We generate the slurm config file using the online tool https://slurm.schedmd.com/configurator.easy.html
The result is copied to /etc/slurm in the master and looks like this:
```
# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=ixtlilton
ControlAddr=192.168.0.100
# 
#MailProg=/bin/mail 
MpiDefault=none
#MpiParams=ports=#-# 
ProctrackType=proctrack/cgroup
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPort=6817 
SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPort=6818 
SlurmdSpoolDir=/var/spool/slurmd
SlurmUser=slurm
#SlurmdUser=root 
StateSaveLocation=/var/spool/slurmctld
SwitchType=switch/none
TaskPlugin=task/none
# 
# 
# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
# 
# 
# SCHEDULING 
FastSchedule=1
SchedulerType=sched/backfill
SelectType=select/linear
#SelectTypeParameters=
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageType=accounting_storage/none
ClusterName=Ixtlilton
#JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=3 
SlurmctldLogFile=/var/log/slurmctld.log
#SlurmdDebug=3 
SlurmdLogFile=/var/log/slurmd.log
# 
# 
# COMPUTE NODES
NodeName=ixtlilton NodeAddr=192.168.0.100 CPUs=20 RealMemory=62000 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 State=UNKNOWN 
NodeName=node01 NodeAddr=192.168.0.1 CPUs=20 RealMemory=62000 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 State=UNKNOWN
NodeName=node02 NodeAddr=192.168.0.2 CPUs=20 RealMemory=62000 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 State=UNKNOWN
NodeName=node03 NodeAddr=192.168.0.3 CPUs=20 RealMemory=62000 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 State=UNKNOWN
NodeName=node04 NodeAddr=192.168.0.4 CPUs=20 RealMemory=62000 Sockets=2 CoresPerSocket=10 ThreadsPerCore=1 State=UNKNOWN 
PartitionName=debug Nodes=node0[1-4] Default=YES MaxTime=INFINITE State=UP
```
The file needs to be copied all around:
```
cluster_exec 'scp ixtlilton:/etc/slurm/slurm.conf /etc/slurm/slurm.conf'
```
Finnal details in the master:
```bash
mkdir /var/spool/slurmctld
chown slurm: /var/spool/slurmctld
chmod 755 /var/spool/slurmctld
touch /var/log/slurmctld.log
chown slurm: /var/log/slurmctld.log
touch /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
chown slurm: /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
mkdir /var/spool/slurmd
chown slurm: /var/spool/slurmd
chmod 755 /var/spool/slurmd
touch /var/log/slurmd.log
chown slurm: /var/log/slurmd.log
```
Finnal details in slave nodes:
```bash
cluster_exec 'mkdir /var/spool/slurmd; chown slurm: /var/spool/slurmd; chmod 755 /var/spool/slurmd'
cluster_exec 'touch /var/log/slurmd.log; chown slurm: /var/log/slurmd.log'
```

To test configuration:
```bash
slurmd -C; cluster_exec 'slurmd -C'
```

Now we create cgroup.conf and cgroup_allowed_devices_file.conf in everynode. At this moment the configuration was taken from the examples /etc/slurm/cgroup.conf.example and /etc/slurm/cgroup_allowed_devices_file.conf.example

/etc/slurm/cgroup.conf
```CgroupMountpoint="/sys/fs/cgroup"
CgroupReleaseAgentDir="/etc/slurm/cgroup"
AllowedDevicesFile="/etc/slurm/cgroup_allowed_devices_file.conf"
CgroupAutomount=yes

ConstrainDevices=yes
ConstrainCores=no
ConstrainRAMSpace=yes
ConstrainSwapSpace=no
```

```
cluster_exec 'scp ixtlilton:/etc/slurm/cgroup.conf /etc/slurm/cgroup.conf'
```

cgroup_allowed_devices_file.conf:

```
/dev/null
/dev/urandom
/dev/zero
/dev/sda*
/dev/cpu/*/*
/dev/pts/*
```

```
cluster_exec 'scp ixtlilton:/etc/slurm/cgroup_allowed_devices_file.conf /etc/slurm/cgroup_allowed_devices_file.conf'
```


The firewall has to be stopped everywhere (or at least in any slave node see: https://www.slothparadise.com/how-to-install-slurm-on-centos-7-cluster/). I do it everywhere, just in case.
```
systemctl stop firewalld; systemctl disable firewalld
cluster_exec 'systemctl stop firewalld; systemctl disable firewalld'
```

Checking clocks are syncronized along the cluster:
```bash
cluster_exec 'date'; date
cluster_exec 'clock'; clock
```

Now we can try to wake Slurm up.
On every node:
```bash
systemctl enable slurmd.service
systemctl start slurmd.service
systemctl status slurmd.service
cluster_exec 'systemctl enable slurmd.service'
cluster_exec 'systemctl start slurmd.service'
cluster_exec 'systemctl status slurmd.service'
```
On the master:
```
systemctl enable slurmctld.service
systemctl start slurmctld.service
systemctl status slurmctld.service
```

## Administration

Diplaying compute nodes:
```
scontrol show nodes
``` 

To display the job queue:
```
scontrol show jobs
```

```
scontrol show daemons
salloc
sinfo
squeue
smap
sview
scontrol show config
sstat
sreport
```

### Daemons

```bash
scontrol show daemons
```

```bash
systemctl status slurmd.service
```

### Show the partitions

```bash
sinfo
```

```bash
scontrol show partitions
```
### Updating the configuration and restarting the daemons

See script update_and_restart_slurmd.sh.

### How to add a new node

Stop slurmctld
Add/remove nodes in slurm.conf
update_and_restart_slurmd

### How to create a new partition

Edit the file '/etc/slurm/slurm.conf' and in the section '#PARTITIONS' add the corresponding new
line:

```
PartitionName=tests Nodes=ixtlilton Default=YES PriorityJobFactor=20000 MaxTime=INFINITE State=UP OverSubscribe=NO Shared=Yes
PartitionName=mds Nodes=node01,node02,node03,node04 Default=NO PriorityJobFactor=10000 MaxTime=INFINITE State=UP OverSubscribe=NO Shared=Yes
PartitionName=gpus Nodes=ixtlilton,node01,node02,node03,node04 Default=NO PriorityJobFactor=10000 MaxTime=INFINITE State=UP OverSubscribe=NO Shared=Yes
PartitionName=guests Nodes=node04 Default=NO PriorityJobFactor=10000 MaxTime=168:00:00 State=UP OverSubscribe=NO Shared=Yes
```

For the changes in the configuration file to take effect:

```bash
update_and_restart_slurmd
```

### Limiting users access to partitions

Access to partitions can be limited to unix groups. In the partitions definition section of
'/etc/slurm.conf' a new variable must be added to the partition with the number of the groups comma
separated:

```
PartitionName=tests Nodes=ixtlilton Default=YES PriorityJobFactor=20000 MaxTime=INFINITE State=UP OverSubscribe=NO Shared=Yes AllowGroups=liliana,diego
PartitionName=mds Nodes=node01,node02,node03,node04 Default=NO PriorityJobFactor=10000 MaxTime=INFINITE State=UP OverSubscribe=NO Shared=Yes AllowGroups=liliana,diego
PartitionName=gpus Nodes=ixtlilton,node01,node02,node03,node04 Default=NO PriorityJobFactor=10000 MaxTime=INFINITE State=UP OverSubscribe=NO Shared=Yes AllowGroups=liliana,diego
PartitionName=guests Nodes=node04 Default=NO PriorityJobFactor=10000 MaxTime=168:00:00 State=UP OverSubscribe=NO Shared=Yes AllowGroups=guest
```

After editing the file, the slurm daemon needs to be restarted:

```bash
update_and_restart_slurmd
update_and_restart_slurmd
```


> **_More Info:_**    
> https://slurm.schedmd.com/
> https://wiki.fysik.dtu.dk/niflheim/Slurm_configuration     
> https://www.dkrz.de/up/systems/mistral/running-jobs/slurm-introduction
> http://bioinformatics-core-shared-training.github.io/hpc/SLURM-Summary.pdf
> https://github.com/B-UMMI/INNUENDO/wiki/4.-Setting-up-SLURM-partitions-and-nodes
> https://vsoch.github.io/lessons/sherlock-jobs/
> 
